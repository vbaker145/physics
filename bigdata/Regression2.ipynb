{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nonlinear Regression\n",
    "G. Richards (2016), based on materials from Connolly (especially) and Ivezic.\n",
    "\n",
    "Up to now we have been fitting using *linear* models.  Before moving on to *non-linear* models, we'll look at local linear fitting.  Kernel Regression (or Nadaraya-Watson) was one such method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Locally Linear Regression (LOWESS or LOESS)\n",
    "\n",
    "In [Local Linear Regression](https://en.wikipedia.org/wiki/Local_regression) we assume that the regression function at any point can\n",
    "be approximated by a Taylor series expansion.  If we truncated the Taylor series at the first term, then this would be the same as Nadaraya-Watson.\n",
    "\n",
    "This is similar to Kernel regression, except that we fit the local regression to the weighted points\n",
    "\n",
    "$$\\sum_{i=1}^N  K\\left(\\frac{||x-x_i||}{h}\\right) \\left( y_i - w(x) \\, x_i \\right)^2.$$\n",
    "\n",
    "One version of this called LOWESS (locally weighted scatter plot smoothing) uses the \"tricubic\" Kernel:\n",
    "\n",
    "$$K(x_i,x) = \\left ( 1 - \\left ( \\frac{|x - x_i |}h{}\\right )^3 \\right )^3.$$\n",
    "\n",
    "However, the book isn't really very clear on this (or LOESS, which might stand for LOcal regrESSion) and it doesn't appear that this is implemented in either astroML or Scikit-Learn.  So, we are going to move on--just realize that there algorithms that are intermediate between linear and nonlinear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear Regression\n",
    "\n",
    "Often we can make our non-linear data linear (e.g., by taking the log), but that has its own set of complications (e.g., asymmetric error bars).  So we should also consider non-linear regression.\n",
    "\n",
    "If we know the theoretical form of the model, then one option is to use MCMC techniques to sample the parameter space and find the optimal model parameters.\n",
    "\n",
    "An alternate approach is to use the Levenberg-Marquardt (LM) algorithm to optimize the maximum likelihood estimation. [Numerical Recipes](http://numerical.recipes/) is an excellent resource for more information about LM.  I can't really emphasize enough how ubiquitous LM is, you really should learn how it works in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For now let's leave it as these few words of explanation (with links for further study).\n",
    "LM searches through a combination of [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) and [Gauss-Newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) optimization. If we can express our\n",
    "regression function as a Taylor series expansion then, to first order,\n",
    "then we can write\n",
    "\n",
    "$$f(x_i|\\theta) = f(x_i|\\theta_0) + J d\\theta.$$\n",
    "\n",
    "Here $\\theta_0$ is an initial guess for the regression parameters,\n",
    "$J$ is the Jacobian about this point ($J=\\partial f(x_i|\\theta)/ \\partial\n",
    " \\theta$), and $d\\theta$ is a perturbation in the regression\n",
    "parameters. \n",
    "\n",
    "LM minimizes the sum of square errors,\n",
    "\n",
    "$$\\sum_i (y_i- f(x_i|\\theta_0) - J_i d\\theta)^2,$$\n",
    "\n",
    "for a perturbation $d\\theta$. This minimization results in an update relation for\n",
    "$d\\theta$ given by\n",
    "\n",
    "$$(J^TC^{-1}J + \\lambda\\ {\\rm diag}(J^TC^{-1}J) )\\,d\\theta = J^TC^{-1}(Y-f(X|\\theta)),$$\n",
    "\n",
    "where $\\lambda$ term acts as a damping parameter.  If $\\lambda$ is small, then the relation approximates a Gauss-Newton method (i.e., it minimizes the parameters assuming the function is quadratic). If $\\lambda$ is large the perturbation $d\\theta$ follows the direction of\n",
    "steepest descent. The diag$(J^TC^{-1}J)$ term is what makes it different from Ridge Regression and it ensures that the update of $d\\theta$ is largest along directions where the gradient is smallest (which\n",
    "improves convergence).\n",
    "\n",
    "This is an iterative process which ceases when the change in likelihood values reaches a predetermined limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In SciPy [`scipy.optimize.leastsq`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html) implements the LM algorithm.\n",
    "Here is an example call to estimate the first 6 terms of the Taylor series for $y=\\sin x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "x = np.linspace(-3,3,100) # 100 values between -3 and 3\n",
    "\n",
    "def taylor_err(a, x, f):\n",
    "    p = np.arange(len(a))[:, None]\n",
    "    return f(x) - np.dot(a,x**p)\n",
    "\n",
    "a_start = np.zeros(6) # starting guess\n",
    "a_best, flat = optimize.leastsq(taylor_err, a_start, args=(x,np.sin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outliers\n",
    "\n",
    "To be honest, I'm not all all certain why the book brings up outliers at this particular point.  However, we need to talk about outliers sometime.  As with other things today, we'll skip over a lot and do just enough to give you a feel for what can be done.\n",
    "\n",
    "We'll use what we learned from Chapter 5 to adopt a Bayesian approach to identifying outliers and account for them in our fit.\n",
    "\n",
    "Let's assume the data are drawn from two Gaussians distributions (one for the function and the other for the outliers)\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "&  p(\\{y_i\\}|\\{x_i\\}, \\{\\sigma_i\\}, \\theta_0, \\theta_1, \\mu_b, V_b, p_b)\n",
    "  \\propto \\nonumber\\\\\n",
    "&  \\prod_{i=1}^{N} \\bigg[\n",
    "    \\frac{1-p_b}{\\sqrt{2\\pi\\sigma_i^2}}\n",
    "      \\exp\\left(-\\frac{(y_i - \\theta_1 x_i - \\theta_0)^2}\n",
    "               {2 \\sigma_i^2}\\right)\n",
    "    + \\frac{p_b}{\\sqrt{2\\pi(V_b + \\sigma_i^2)}}\n",
    "    \\exp\\left(-\\frac{(y_i - \\mu_b)^2}{2(V_b + \\sigma_i^2)}\\right)\n",
    "    \\bigg].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$V_b$ is the variance of the outlier distribution. If we use MCMC we can marginalize over the nuisance  parameters $p_b$, $V_b$, $\\mu_b$. We could also calculate the probability that a point is drawn from the outlier or \"model\" Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To do the example below, you'll need [`PyMC`](http://pymc-devs.github.io/pymc/), which you can install with:\n",
    "```\n",
    "conda install -c anaconda pymc\n",
    "```\n",
    "\n",
    "The top-left panel shows the data, including 3 obvious outlier points.  Not accounting for the outliers gives the dotted line and the model parameters shown in the top right.  Accounting for the outliers with two different methods gives the dashed and solid lines in the top left and the parameter fits given in the bottom 2 plots.\n",
    "\n",
    "What is going on here is beyond the scope of what we have time to get into for this class, but I wanted you to be aware that there are tools/methods to handle such cases (including pyMC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "# Hack to fix import issue in older versions of pymc\n",
    "import scipy\n",
    "import scipy.misc\n",
    "scipy.derivative = scipy.misc.derivative\n",
    "import pymc\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data: this includes outliers\n",
    "data = fetch_hogg2010test()\n",
    "xi = data['x']\n",
    "yi = data['y']\n",
    "dyi = data['sigma_y']\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# First model: no outlier correction\n",
    "# define priors on beta = (slope, intercept)\n",
    "@pymc.stochastic\n",
    "def beta_M0(value=np.array([2., 100.])):\n",
    "    \"\"\"Slope and intercept parameters for a straight line.\n",
    "    The likelihood corresponds to the prior probability of the parameters.\"\"\"\n",
    "    slope, intercept = value\n",
    "    prob_intercept = 1 + 0 * intercept\n",
    "    # uniform prior on theta = arctan(slope)\n",
    "    # d[arctan(x)]/dx = 1 / (1 + x^2)\n",
    "    prob_slope = np.log(1. / (1. + slope ** 2))\n",
    "    return prob_intercept + prob_slope\n",
    "\n",
    "\n",
    "@pymc.deterministic\n",
    "def model_M0(xi=xi, beta=beta_M0):\n",
    "    slope, intercept = beta\n",
    "    return slope * xi + intercept\n",
    "\n",
    "y = pymc.Normal('y', mu=model_M0, tau=dyi ** -2,\n",
    "                observed=True, value=yi)\n",
    "\n",
    "M0 = dict(beta_M0=beta_M0, model_M0=model_M0, y=y)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Second model: nuisance variables correcting for outliers\n",
    "# This is the mixture model given in equation 17 in Hogg et al\n",
    "\n",
    "# define priors on beta = (slope, intercept)\n",
    "@pymc.stochastic\n",
    "def beta_M1(value=np.array([2., 100.])):\n",
    "    \"\"\"Slope and intercept parameters for a straight line.\n",
    "    The likelihood corresponds to the prior probability of the parameters.\"\"\"\n",
    "    slope, intercept = value\n",
    "    prob_intercept = 1 + 0 * intercept\n",
    "    # uniform prior on theta = arctan(slope)\n",
    "    # d[arctan(x)]/dx = 1 / (1 + x^2)\n",
    "    prob_slope = np.log(1. / (1. + slope ** 2))\n",
    "    return prob_intercept + prob_slope\n",
    "\n",
    "\n",
    "@pymc.deterministic\n",
    "def model_M1(xi=xi, beta=beta_M1):\n",
    "    slope, intercept = beta\n",
    "    return slope * xi + intercept\n",
    "\n",
    "# uniform prior on Pb, the fraction of bad points\n",
    "Pb = pymc.Uniform('Pb', 0, 1.0, value=0.1)\n",
    "\n",
    "# uniform prior on Yb, the centroid of the outlier distribution\n",
    "Yb = pymc.Uniform('Yb', -10000, 10000, value=0)\n",
    "\n",
    "# uniform prior on log(sigmab), the spread of the outlier distribution\n",
    "log_sigmab = pymc.Uniform('log_sigmab', -10, 10, value=5)\n",
    "\n",
    "\n",
    "@pymc.deterministic\n",
    "def sigmab(log_sigmab=log_sigmab):\n",
    "    return np.exp(log_sigmab)\n",
    "\n",
    "\n",
    "# set up the expression for likelihood\n",
    "def mixture_likelihood(yi, model, dyi, Pb, Yb, sigmab):\n",
    "    \"\"\"Equation 17 of Hogg 2010\"\"\"\n",
    "    Vi = dyi ** 2\n",
    "    Vb = sigmab ** 2\n",
    "\n",
    "    root2pi = np.sqrt(2 * np.pi)\n",
    "\n",
    "    L_in = (1. / root2pi / dyi\n",
    "            * np.exp(-0.5 * (yi - model) ** 2 / Vi))\n",
    "\n",
    "    L_out = (1. / root2pi / np.sqrt(Vi + Vb)\n",
    "             * np.exp(-0.5 * (yi - Yb) ** 2 / (Vi + Vb)))\n",
    "\n",
    "    return np.sum(np.log((1 - Pb) * L_in + Pb * L_out))\n",
    "\n",
    "MixtureNormal = pymc.stochastic_from_dist('mixturenormal',\n",
    "                                          logp=mixture_likelihood,\n",
    "                                          dtype=np.float,\n",
    "                                          mv=True)\n",
    "\n",
    "y_mixture = MixtureNormal('y_mixture', model=model_M1, dyi=dyi,\n",
    "                          Pb=Pb, Yb=Yb, sigmab=sigmab,\n",
    "                          observed=True, value=yi)\n",
    "\n",
    "M1 = dict(y_mixture=y_mixture, beta_M1=beta_M1, model_M1=model_M1,\n",
    "          Pb=Pb, Yb=Yb, log_sigmab=log_sigmab, sigmab=sigmab)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Third model: marginalizes over the probability that each point is an outlier.\n",
    "# define priors on beta = (slope, intercept)\n",
    "@pymc.stochastic\n",
    "def beta_M2(value=np.array([2., 100.])):\n",
    "    \"\"\"Slope and intercept parameters for a straight line.\n",
    "    The likelihood corresponds to the prior probability of the parameters.\"\"\"\n",
    "    slope, intercept = value\n",
    "    prob_intercept = 1 + 0 * intercept\n",
    "    # uniform prior on theta = arctan(slope)\n",
    "    # d[arctan(x)]/dx = 1 / (1 + x^2)\n",
    "    prob_slope = np.log(1. / (1. + slope ** 2))\n",
    "    return prob_intercept + prob_slope\n",
    "\n",
    "\n",
    "@pymc.deterministic\n",
    "def model_M2(xi=xi, beta=beta_M2):\n",
    "    slope, intercept = beta\n",
    "    return slope * xi + intercept\n",
    "\n",
    "# qi is bernoulli distributed\n",
    "# Note: this syntax requires pymc version 2.2\n",
    "qi = pymc.Bernoulli('qi', p=1 - Pb, value=np.ones(len(xi)))\n",
    "\n",
    "\n",
    "def outlier_likelihood(yi, mu, dyi, qi, Yb, sigmab):\n",
    "    \"\"\"likelihood for full outlier posterior\"\"\"\n",
    "    Vi = dyi ** 2\n",
    "    Vb = sigmab ** 2\n",
    "\n",
    "    root2pi = np.sqrt(2 * np.pi)\n",
    "\n",
    "    logL_in = -0.5 * np.sum(qi * (np.log(2 * np.pi * Vi)\n",
    "                                  + (yi - mu) ** 2 / Vi))\n",
    "\n",
    "    logL_out = -0.5 * np.sum((1 - qi) * (np.log(2 * np.pi * (Vi + Vb))\n",
    "                                         + (yi - Yb) ** 2 / (Vi + Vb)))\n",
    "\n",
    "    return logL_out + logL_in\n",
    "\n",
    "OutlierNormal = pymc.stochastic_from_dist('outliernormal',\n",
    "                                          logp=outlier_likelihood,\n",
    "                                          dtype=np.float,\n",
    "                                          mv=True)\n",
    "\n",
    "y_outlier = OutlierNormal('y_outlier', mu=model_M2, dyi=dyi,\n",
    "                          Yb=Yb, sigmab=sigmab, qi=qi,\n",
    "                          observed=True, value=yi)\n",
    "\n",
    "M2 = dict(y_outlier=y_outlier, beta_M2=beta_M2, model_M2=model_M2,\n",
    "          qi=qi, Pb=Pb, Yb=Yb, log_sigmab=log_sigmab, sigmab=sigmab)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the data\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.25,\n",
    "                    bottom=0.1, top=0.95, hspace=0.2)\n",
    "\n",
    "# first axes: plot the data\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.errorbar(xi, yi, dyi, fmt='.k', ecolor='gray', lw=1)\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Go through models; compute and plot likelihoods\n",
    "models = [M0, M1, M2]\n",
    "linestyles = [':', '--', '-']\n",
    "labels = ['no outlier correction\\n(dotted fit)',\n",
    "          'mixture model\\n(dashed fit)',\n",
    "          'outlier rejection\\n(solid fit)']\n",
    "\n",
    "\n",
    "x = np.linspace(0, 350, 10)\n",
    "\n",
    "bins = [(np.linspace(140, 300, 51), np.linspace(0.6, 1.6, 51)),\n",
    "        (np.linspace(-40, 120, 51), np.linspace(1.8, 2.8, 51)),\n",
    "        (np.linspace(-40, 120, 51), np.linspace(1.8, 2.8, 51))]\n",
    "\n",
    "for i, M in enumerate(models):\n",
    "    S = pymc.MCMC(M)\n",
    "    S.sample(iter=25000, burn=5000)\n",
    "    trace = S.trace('beta_M%i' % i)\n",
    "\n",
    "    H2D, bins1, bins2 = np.histogram2d(trace[:, 0], trace[:, 1], bins=50)\n",
    "    w = np.where(H2D == H2D.max())\n",
    "\n",
    "    # choose the maximum posterior slope and intercept\n",
    "    slope_best = bins1[w[0][0]]\n",
    "    intercept_best = bins2[w[1][0]]\n",
    "\n",
    "    # plot the best-fit line\n",
    "    ax1.plot(x, intercept_best + slope_best * x, linestyles[i], c='k')\n",
    "\n",
    "    # For the model which identifies bad points,\n",
    "    # plot circles around points identified as outliers.\n",
    "    if i == 2:\n",
    "        qi = S.trace('qi')[:]\n",
    "        Pi = qi.astype(float).mean(0)\n",
    "        outlier_x = xi[Pi < 0.32]\n",
    "        outlier_y = yi[Pi < 0.32]\n",
    "        ax1.scatter(outlier_x, outlier_y, lw=1, s=400, alpha=0.5,\n",
    "                    facecolors='none', edgecolors='red')\n",
    "\n",
    "    # plot the likelihood contours\n",
    "    ax = plt.subplot(222 + i)\n",
    "\n",
    "    H, xbins, ybins = np.histogram2d(trace[:, 1], trace[:, 0], bins=bins[i])\n",
    "    H[H == 0] = 1E-16\n",
    "    Nsigma = convert_to_stdev(np.log(H))\n",
    "\n",
    "    ax.contour(0.5 * (xbins[1:] + xbins[:-1]),\n",
    "               0.5 * (ybins[1:] + ybins[:-1]),\n",
    "               Nsigma.T, levels=[0.683, 0.955], colors='black')\n",
    "\n",
    "    ax.set_xlabel('intercept')\n",
    "    ax.set_ylabel('slope')\n",
    "    ax.grid(color='gray')\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(40))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "\n",
    "    ax.text(0.98, 0.98, labels[i], ha='right', va='top',\n",
    "            bbox=dict(fc='w', ec='none', alpha=0.5),\n",
    "            transform=ax.transAxes)\n",
    "    ax.set_xlim(bins[i][0][0], bins[i][0][-1])\n",
    "    ax.set_ylim(bins[i][1][0], bins[i][1][-1])\n",
    "\n",
    "ax1.set_xlim(0, 350)\n",
    "ax1.set_ylim(100, 700)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Proccess Regression\n",
    "\n",
    "\n",
    "A [Gaussian Process](https://en.wikipedia.org/wiki/Gaussian_process) (GP) is a collection of random variables in a parameter space for which any subset can be defined by a joint Gaussian distribution. \n",
    "\n",
    "I don't (yet!) fully understand what is going on here either.  But here's my current understanding.\n",
    "\n",
    "In the top-left panel below, we have drawn some random distributions from a Gaussian Basis.  Specifically, we have put down evenly spaced Gaussians across the parameter space, that have width of $h$ and covariance given by \n",
    "\n",
    "$${\\rm Cov}(x_1, x_2; h) = \\exp\\left(\\frac{-(x_1 - x_2)^2}{2 h^2}\\right).$$\n",
    "\n",
    "For a given bandwidth we can obviously define an infinite set of such functions.\n",
    "\n",
    "Then in the top-right panel, we constrain these functions by selecting those that pass though a given set of points using the posterior:\n",
    "\n",
    "$$p(f_j | \\{x_i, y_i, \\sigma_i\\}, x_j^\\ast).$$\n",
    "\n",
    "The bottom panels show the result for the same points with error bars and 20 noisy points drawn from $y=\\cos(x)$.  You can perhaps see how this might be useful.\n",
    "\n",
    "![Ivezic, Figure 8.10](http://www.astroml.org/_images/fig_gp_example_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the code that produced that plot (Ivezic, Figure 8.10).  See what happens if you make the number of Gaussians much smaller or much bigger, or if you change the bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 8.10\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# define a squared exponential covariance function\n",
    "def squared_exponential(x1, x2, h):\n",
    "    return np.exp(-0.5 * (x1 - x2) ** 2 / h ** 2)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# draw samples from the unconstrained covariance\n",
    "np.random.seed(1)\n",
    "x = np.linspace(0, 10, 100)\n",
    "h = 1.0\n",
    "\n",
    "mu = np.zeros(len(x))\n",
    "C = squared_exponential(x, x[:, None], h)\n",
    "draws = np.random.multivariate_normal(mu, C, 3)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with two points\n",
    "x1 = np.array([2.5, 7])\n",
    "y1 = np.cos(x1)\n",
    "gp1 = GaussianProcess(corr='squared_exponential', theta0=0.5, random_state=0)\n",
    "gp1.fit(x1[:, None], y1)\n",
    "f1, MSE1 = gp1.predict(x[:, None], eval_MSE=True)\n",
    "f1_err = np.sqrt(MSE1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with two noisy points\n",
    "#  scikit-learn gaussian process uses nomenclature from the geophysics\n",
    "#  community, where a \"nugget\" can be specified.  The diagonal of the\n",
    "#  assumed covariance matrix is multiplied by the nugget.  This is\n",
    "#  how the error on inputs is incorporated into the calculation\n",
    "dy2 = 0.2\n",
    "gp2 = GaussianProcess(corr='squared_exponential', theta0=0.5, nugget=(dy2 / y1) ** 2, random_state=0)\n",
    "gp2.fit(x1[:, None], y1)\n",
    "f2, MSE2 = gp2.predict(x[:, None], eval_MSE=True)\n",
    "f2_err = np.sqrt(MSE2)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with many noisy points\n",
    "x3 = np.linspace(0, 10, 20)\n",
    "y3 = np.cos(x3)\n",
    "dy3 = 0.2\n",
    "y3 = np.random.normal(y3, dy3)\n",
    "gp3 = GaussianProcess(corr='squared_exponential', theta0=0.5,\n",
    "                      thetaL=0.01, thetaU=10.0,\n",
    "                      nugget=(dy3 / y3) ** 2,\n",
    "                      random_state=0)\n",
    "gp3.fit(x3[:, None], y3)\n",
    "f3, MSE3 = gp3.predict(x[:, None], eval_MSE=True)\n",
    "f3_err = np.sqrt(MSE3)\n",
    "\n",
    "# we have fit for the `h` parameter: print the result here:\n",
    "print \"best-fit theta (bandwidth)=\", gp3.theta_[0]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the diagrams\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "# first: plot a selection of unconstrained functions\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(x, draws.T, '-k')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "\n",
    "# second: plot a constrained function\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(x, f1, '-', color='gray')\n",
    "ax.fill_between(x, f1 - 2 * f1_err, f1 + 2 * f1_err, color='gray', alpha=0.3)\n",
    "ax.plot(x1, y1, '.k', ms=6)\n",
    "\n",
    "\n",
    "# third: plot a constrained function with errors\n",
    "ax = fig.add_subplot(223)\n",
    "ax.plot(x, f2, '-', color='gray')\n",
    "ax.fill_between(x, f2 - 2 * f2_err, f2 + 2 * f2_err, color='gray', alpha=0.3)\n",
    "ax.errorbar(x1, y1, dy2, fmt='.k', ms=6)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "\n",
    "# third: plot a more constrained function with errors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(x, f3, '-', color='gray')\n",
    "ax.fill_between(x, f3 - 2 * f3_err, f3 + 2 * f3_err, color='gray', alpha=0.3)\n",
    "ax.errorbar(x3, y3, dy3, fmt='.k', ms=6)\n",
    "\n",
    "ax.plot(x, np.cos(x), ':k')\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlim(0, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For GP regression we want to estimate the value and variance of a new set of points given an input data set. This is equivalent to averaging over all functions that pass through our input data\n",
    "\n",
    "The covariance matrix\n",
    "\n",
    "> $  K = \\begin{pmatrix}\n",
    "    K_{11} & K_{12} \\\\\n",
    "    K_{12}^T & K_{22}\n",
    "  \\end{pmatrix},\n",
    "$\n",
    "\n",
    "where $K_{11}$ is the covariance between the input points $x_i$ with\n",
    "observational errors $\\sigma_i^2$ added in quadrature to the diagonal,\n",
    "$K_{12}$ is\n",
    "the cross-covariance between the input points $x_i$ and the unknown points\n",
    "$x^\\ast_j$, and $K_{22}$ is the covariance between the unknown points\n",
    "$x_j^\\ast$.  Then for observed vectors $\\vec{x}$ and $\\vec{y}$, and a vector\n",
    "of unknown points $\\vec{x}^\\ast$, it can be shown that the posterior is given by\n",
    "\n",
    ">$  p(f_j | \\{x_i, y_i, \\sigma_i\\}, x_j^\\ast) = \\mathcal{N}(\\vec{\\mu}, \\Sigma)$\n",
    "\n",
    "where\n",
    "\n",
    ">$\n",
    "\\begin{eqnarray}\n",
    "  \\vec{\\mu} &=& K_{12} K_{11}^{-1} \\vec{y}, \\\\\n",
    "  \\Sigma &=& K_{22} - K_{12}^TK_{11}^{-1}K_{12}\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "$\\mu_j$ gives the expected value $\\bar{f}^\\ast_j$ of the result, and\n",
    "$\\Sigma_{jk}$ gives the error covariance between any two unknown points.\n",
    "\n",
    "_it gives the value and uncertainty of a predicted point_\n",
    "\n",
    "Note that the physics of the underlying process enters through the assumed\n",
    "form of the covariance function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apparently the \"bible\" for GP is [Rasmussen and Williams \"Gaussian Processes for Machine Learning\" (2005)](http://www.gaussianprocess.org/gpml/).\n",
    "\n",
    "The Scikit-Learn [`GaussianProcess`](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcess.html) implementation looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "X = np.random.random((100,2))\n",
    "y = np.sin(10*X[:,0] + X[:,1])\n",
    "gp = GaussianProcess(corr='squared_exponential')\n",
    "gp.fit(X,y)\n",
    "y_pred, dy_pred = gp.predict(X, eval_MSE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below we see what GP does for the supernova example that we used last time.   What is great is that not only do you get a fit, you get errors and can tell where the fit is good and where it is poor.\n",
    "\n",
    "This looks to be pretty useful for time-domain data too, so maybe we'll come back to it after next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 8.11\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "\n",
    "from astroML.cosmology import Cosmology\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate data\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "cosmo = Cosmology()\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = np.asarray(map(cosmo.mu, z))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# fit the data\n",
    "# Mesh the input space for evaluations of the real function,\n",
    "# the prediction and its MSE\n",
    "z_fit = np.linspace(0, 2, 1000)\n",
    "gp = GaussianProcess(corr='squared_exponential', theta0=1e-1,\n",
    "                     thetaL=1e-2, thetaU=1,\n",
    "                     normalize=False,\n",
    "                     nugget=(dmu / mu_sample) ** 2,\n",
    "                     random_start=1)\n",
    "gp.fit(z_sample[:, None], mu_sample)\n",
    "y_pred, MSE = gp.predict(z_fit[:, None], eval_MSE=True)\n",
    "sigma = np.sqrt(MSE)\n",
    "print \"Optimal bandwidth=\", gp.theta_[0,0]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the gaussian process\n",
    "#  gaussian process allows computation of the error at each point\n",
    "#  so we will show this as a shaded region\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.1, top=0.95)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_true, '--k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', markersize=6)\n",
    "ax.plot(z_fit, y_pred, '-k')\n",
    "ax.fill_between(z_fit, y_pred - 1.96 * sigma, y_pred + 1.96 * sigma,\n",
    "                alpha=0.2, color='b', label='95% confidence interval')\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(36, 48)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "N.B.  In this process we are assuming that the $x$ values are error free.  But really they will have error too.  Ivezic $\\S$ 8.8 deals with this, but we are skipping it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Over-/Under-Fitting\n",
    "\n",
    "We already talked a little bit about overfitting, but let's dive down deeper.  We'll use a 1-D model with homoscedastic errors for the sake of illustration, but this discussion applies to more complicated data as well.\n",
    "\n",
    "To be clear, our data consists of $X_{\\rm train}$, $y_{\\rm train}$, and $X_{\\rm test}$ and we are trying to predict $y_{\\rm test}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take an example where\n",
    "$$0\\le x_i \\le 3$$\n",
    "and\n",
    "$$y_i = x_i \\sin(x_i) + \\epsilon_i,$$\n",
    "where the noise, $\\epsilon_i$ is given by $\\mathscr{N}(0,0.1)$.\n",
    "\n",
    "In the example below, we draw 20 evenly spaced points from this distribution and fit them with a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 8.12\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib.patches import FancyArrow\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# First figure: plot points with a linear fit\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(x, y, marker='x', c='k', s=30)\n",
    "\n",
    "p = np.polyfit(x, y, 1)\n",
    "y_fit = np.polyval(p, x_fit)\n",
    "\n",
    "ax.text(0.03, 0.96, \"d = 1\", transform=plt.gca().transAxes,\n",
    "        ha='left', va='top',\n",
    "        bbox=dict(ec='k', fc='w', pad=10))\n",
    "\n",
    "ax.plot(x_fit, y_fit, '-b')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This model underfits the data and is said to be \"biased\" (in the sense that the estimated model parameters deviate significantly from the true model parameters).  \n",
    "\n",
    "A straight line is a polynomial of order 1, so let's try polynomials of higher order.  Specifically, 2, 3, and all the way up to 19.  We see that 3 is pretty good in that it seems to be relatively unbiased and also lacks the high variance of the order=19 fit (which is also unbiased, but overfit).  So, how do we choose the \"right\" answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 8.13\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib.patches import FancyArrow\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Second figure: plot fit for several orders of polynomial\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.subplots_adjust(wspace=0.03, bottom=0.15,\n",
    "                    top=0.95, left=0.07, right=0.97)\n",
    "\n",
    "for i, d in enumerate([2, 3, 19]):\n",
    "    ax = fig.add_subplot(131 + i)\n",
    "    ax.scatter(x, y, marker='x', c='k', s=30)\n",
    "\n",
    "    p = np.polyfit(x, y, d)\n",
    "    y_fit = np.polyval(p, x_fit)\n",
    "\n",
    "    ax.plot(x_fit, y_fit, '-b')\n",
    "    ax.set_ylim(-0.1, 2.1)\n",
    "    ax.set_xlim(-0.2, 3.2)\n",
    "    if i in (1, 2):\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.set_ylabel('$y$')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.text(0.08, 0.94, \"d = %i\" % d, transform=ax.transAxes,\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(ec='k', fc='w', pad=10))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "Determining the best trade-off between bias and variance is done through [cross-validation](https://en.wikipedia.org/wiki/Cross-validation).\n",
    "\n",
    "When we increase the complexity of a model, the data points fit the model more and more closely.   However, this process does not necessarily result in a better fit to the data.  Rather, if the order is too hight, then we are *overfitting* the data.  The model has high variance, meaning that a small change in a training point can change the model dramatically.  \n",
    "\n",
    "We can evaluate this using a training set (50-70% of sample),  a cross-validation set (15-25%) and a test set (15-25%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The training set is used the determine the model paramters, $\\theta_j$.  The training data and cross-validation data then are both used to evaluate the training and cross-validation errors ($\\epsilon_{\\rm tr}$ and $\\epsilon_{\\rm CV}$):\n",
    "\n",
    "$$\\epsilon_{\\rm cv/tr} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{N_{\\rm cv/tr}}\n",
    "  \\left[y_i - \\sum_{m=0}^d \\theta_0^{(n)}x_i^m\\right]}$$\n",
    "\n",
    "Why do we need both a training set and a cross-validation set?  Well, because the model parameters, $\\theta_j$, are learned from the training set, but the \"hyperparameters\" (in this case the order) are learned from the cross-validation set.  \n",
    "\n",
    "The test set then provides the best estimate of the error expected for a new set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We show this graphically in the next figure (Ivezic, 8.14).  For low order both the training and CV error are high--this is sign of a high-bias model that is underfitting the data.  For high order, the training error becomes small, but the CV error is large.   This is the sign of a high-variance model that is overfitting the data.  It is matching the subtle variations in the training data that aren't really real, and this shows up in the CV analysis.\n",
    "\n",
    "Similarly we could look at the AIC or BIC which give similiar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 8.14\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib.patches import FancyArrow\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Select the cross-validation points\n",
    "np.random.seed(1)\n",
    "x_cv = np.random.random(20)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Third figure: plot errors as a function of polynomial degree d\n",
    "d = np.arange(0, 21)\n",
    "training_err = np.zeros(d.shape)\n",
    "crossval_err = np.zeros(d.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i in range(len(d)):\n",
    "    p = np.polyfit(x, y, d[i])\n",
    "    training_err[i] = np.sqrt(np.sum((np.polyval(p, x) - y) ** 2)/ len(y))\n",
    "    crossval_err[i] = np.sqrt(np.sum((np.polyval(p, x_cv) - y_cv) ** 2)/ len(y_cv))\n",
    "\n",
    "BIC_train = np.sqrt(len(y)) * training_err / dy + d * np.log(len(y))\n",
    "BIC_crossval = np.sqrt(len(y)) * crossval_err / dy + d * np.log(len(y))\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(d, crossval_err, '--k', label='cross-validation')\n",
    "ax.plot(d, training_err, '-k', label='training')\n",
    "ax.plot(d, 0.1 * np.ones(d.shape), ':k')\n",
    "\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(d, BIC_crossval, '--k', label='cross-validation')\n",
    "ax.plot(d, BIC_train, '-k', label='training')\n",
    "\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('BIC')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Curves\n",
    "\n",
    "We can use a tool called a learning curve to determine if, for a given model, having more training data would help improve the model fits. \n",
    "\n",
    "There are two regimes:\n",
    "\n",
    "1. The training and CV errors have converged.  This indicates that the model is dominated by bias.  In this case increasing the number of training points is futile.\n",
    "2. The training error is smaller than the CV error.  This indicates that the model is dominated by variance.  In this case, increasing the number of training points may help to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 8.15\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib.patches import FancyArrow\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Select the cross-validation points\n",
    "np.random.seed(1)\n",
    "x_cv = 3 * np.random.random(20)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fourth figure: plot errors as a function of training set size\n",
    "np.random.seed(0)\n",
    "x = 3 * np.random.random(100)\n",
    "y = func(x)\n",
    "\n",
    "np.random.seed(1)\n",
    "x_cv = 3 * np.random.random(100)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "Nrange = np.arange(10, 101, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(left=0.15, top=0.95)\n",
    "\n",
    "for subplot, d in zip([211, 212], [2, 12]):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "    training_err = np.zeros(Nrange.shape)\n",
    "    crossval_err = np.zeros(Nrange.shape)\n",
    "\n",
    "    for j, N in enumerate(Nrange):\n",
    "        p = np.polyfit(x[:N], y[:N], d)\n",
    "        training_err[j] = np.sqrt(np.sum((np.polyval(p, x[:N])\n",
    "                                          - y[:N]) ** 2) / len(y))\n",
    "        crossval_err[j] = np.sqrt(np.sum((np.polyval(p, x_cv)\n",
    "                                          - y_cv) ** 2) / len(y_cv))\n",
    "\n",
    "    ax.plot(Nrange, crossval_err, '--k', label='cross-validation')\n",
    "    ax.plot(Nrange, training_err, '-k', label='training')\n",
    "    ax.plot(Nrange, 0.1 * np.ones(Nrange.shape), ':k')\n",
    "    ax.legend(loc=1)\n",
    "    ax.text(0.03, 0.94, \"d = %i\" % d, transform=ax.transAxes,\n",
    "            ha='left', va='top', bbox=dict(ec='k', fc='w', pad=10))\n",
    "\n",
    "    ax.set_ylim(0, 0.4)\n",
    "\n",
    "    ax.set_xlabel('Number of training points')\n",
    "    ax.set_ylabel('rms error')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Honestly, I find this confusing as both cases converge with enough training points.   For example, I don't see that that means that $d=12$ with 100 training points is biased.  So we'll have to take a closer look at that."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
